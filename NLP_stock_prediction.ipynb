{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = True\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "MODEL = 'cardiffnlp/twitter-xlm-roberta-base-sentiment' #cardiffnlp/twitter-xlm-roberta-base-sentiment  cardiffnlp/twitter-roberta-base-emotion\n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    if len(password) > 0:\n",
    "        kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "        #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "        #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "        del password;\n",
    "        gc.collect();\n",
    "        get_datasets(kaggle_api_key);\n",
    "    elif len(glob('data/*')) < 4:\n",
    "        cont = input(f\"Password is empty. Press n to cancel or any other key to continue: \")\n",
    "        if cont == 'n':\n",
    "            assert False, \"Exiting program. Please enter a password to continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = emotion_classifier_load(MODEL)\n",
    "\n",
    "# classifier and classify the dataset of analyst ratings and partner headlines\n",
    "if not os.path.exists('data/Text/text_emotion_29.parquet'):\n",
    "    text_df = pd.concat([\n",
    "                    load_file('data/Text/analyst_ratings_processed.parquet').drop(columns=['unnamed: 0']).rename(columns={'title': 'text'}), \n",
    "                    load_file('data/Text/raw_partner_headlines.parquet').drop(columns=['unnamed: 0', 'url', 'publisher']).rename(columns={'headline': 'text'})\n",
    "                    ], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "    text=text_df.text.tolist()\n",
    "    text = classify_text(emotion_classifier, text, MODELNAME=MODEL, bs=100, device=cuda)\n",
    "    text_dict = {'emotion': text}\n",
    "    text_df = pd.concat([text_df, pd.DataFrame(text_dict)], axis=1)\n",
    "    save_file(text_df, 'data/Text/text_emotion_29.parquet')\n",
    "    text_df\n",
    "else:\n",
    "    text_df = load_file('data/Text/text_emotion_29.parquet')\n",
    "    text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_stock_data(ticker=None, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Experiments with web scraping. Unfinished, so ignore if it does not work #############################\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "import scrapy\n",
    "import stweet as st\n",
    "import requests\n",
    "from stweet.search_runner import SearchRunContext\n",
    "import tor_python_easy\n",
    "import io\n",
    "#arrow time\n",
    "# import urllib.request as urllib2\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "# import yaml\n",
    "# INSTALL DOCKER COMPOSE\n",
    "\n",
    "# if not os.path.exists('docker-compose.yml'):\n",
    "#     url = 'https://raw.githubusercontent.com/markowanga/tor-python-easy/main/docker-compose.yml'\n",
    "#     filename = 'docker-compose.yml'\n",
    "#     urllib2.urlretrieve(url, filename);\n",
    "#     with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#     config['services']['torproxy']['environment'][0]=f'PASSWORD={os.urandom(32).hex()}';\n",
    "#     #with open('docker-compose.yml', 'w') as file:\n",
    "#        # yaml.dump(config, file);\n",
    "\n",
    "# with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#         tor_pass = config['services']['torproxy']['environment'][0].split('=')[1]; \n",
    "\n",
    "# from tor_python_easy.tor_control_port_client import TorControlPortClient\n",
    "# from tor_python_easy.tor_socks_get_ip_client import TorSocksGetIpClient\n",
    "# proxy_config = {'http': 'socks5://localhost:9050','https': 'socks5://localhost:9050',}\n",
    "# ip_client = TorSocksGetIpClient(proxy_config)\n",
    "# tor_control_port_client = TorControlPortClient(control_address='localhost', control_port=9051, control_password=tor_pass)\n",
    "\n",
    "def url_fetch(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text, article.title, article.publish_date, article.authors, article.summary, article.keywords\n",
    "\n",
    "def crawl_url(url):\n",
    "    page = newspaper.build(url, memoize_articles=False)\n",
    "    articles = page.articles\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = arrow.get('2021-01-01').datetime\n",
    "until = arrow.get('2021-02-01').datetime\n",
    "exact_words = None\n",
    "all_words = None\n",
    "any_word = '#apple'\n",
    "\n",
    "# web_client = st.DefaultTwitterWebClientProvider.get_web_client_preconfigured_for_tor_proxy(\n",
    "#     socks_proxy_url='socks5://localhost:9050',\n",
    "#     control_host='localhost',\n",
    "#     control_password=tor_pass,\n",
    "#     control_port=9051,\n",
    "# )\n",
    "\n",
    "\n",
    "collect1 = st.CollectorRawOutput()\n",
    "collect2 = st.CollectorRawOutput()\n",
    "context = SearchRunContext()\n",
    "\n",
    "#since = arrow.get(since).datetime\n",
    "#until = arrow.get(until).datetime\n",
    "\n",
    "tweets_task = st.SearchTweetsTask(since=since, until=until, any_word=any_word, exact_words=exact_words,all_words=all_words, tweets_limit=50, replies_filter=True)\n",
    "runner = st.TweetSearchRunner(search_tweets_task=tweets_task,tweet_raw_data_outputs=[collect1],user_raw_data_outputs=[collect2], web_client=None, search_run_context=context)\n",
    "\n",
    "runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.search_run_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = collect1.get_raw_list()\n",
    "\n",
    "l2 = collect2.get_raw_list()\n",
    "\n",
    "len(l), len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = [value for key,value in json.loads(l[0].to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']]\n",
    "d[0] = arrow.get(d[0], 'ddd MMM DD HH:mm:ss Z YYYY').datetime.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set([[value for key,value in json.loads(x.to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']][1] for x in l])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
