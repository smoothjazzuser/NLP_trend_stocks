{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from glob import glob\n",
    "from compress_pickle import dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from yahooquery import Ticker\n",
    "import timeit\n",
    "import time\n",
    "import gc, os\n",
    "from time import sleep\n",
    "import datetime\n",
    "from getpass import getpass\n",
    "from shutil import rmtree\n",
    "from utils import fernet_key_encryption, aquire_stock_search_terms as aquire_terms, get_macroeconomic_data as macro_data, download_dataset, save_file, load_file, interpolate_months_to_days, intersect_df, parse_emotion_dataframes, get_emotion_df, create_triplets,  get_datasets, convert_project_files_to_parquet\n",
    "from models import siamese_model_dense, triplet_loss\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers, losses, metrics\n",
    "import keras_tuner as kt\n",
    "from shutil import rmtree\n",
    "\n",
    "# enable memory growth for GPU and mixed precision\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    import subprocess\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    vram = tf.config.experimental.get_memory_growth(physical_devices[0])\n",
    "    print('Memory growth:', vram)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "    print(physical_devices)\n",
    "\n",
    "seed=42\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# use this for reproducible random sampling\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_mem():\n",
    "    \"\"\"Return the available GPU memory in GB.\"\"\"\n",
    "    MB_memory = int(\"\".join([x for x in subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv\"]).decode() if x.isdigit()]))\n",
    "    GB_memory = MB_memory / 1000\n",
    "    return GB_memory\n",
    "\n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this first run, you will only need to enter the password to load the api keys.\n",
    "# If you need to change the keys or password, delete the relevent .secret keys file and run this section again.\n",
    "# salt.secret is a non-sensitive file that is used to both generate the encryption key as well as decryption. If this key is lost, the encrypted files are lost and you will need to re-enter the api keys.\n",
    "\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "\n",
    "    kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "    #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "\n",
    "    #data.nasdaq.com api key through quandle\n",
    "    data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "\n",
    "    del password;\n",
    "    gc.collect();\n",
    "\n",
    "    get_datasets(kaggle_api_key, data_nasdaq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the company info for all the ticker symbols and return a dataframe with relevant search terms for each company.\n",
    "# If the stocks dataset is updated on kaggle, compank_list.pkl needs to be deleted and this run again if the symbols have changed. \n",
    "# TODO: It would be more efficient to manually pull the new stock data ourselves and keep the old ticker symbols.\n",
    "search_terms = aquire_terms('data/Stock/')\n",
    "search_terms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_file('data/Macro/US_macroeconomics.parquet')\n",
    "df2 = load_file('data\\Stock\\AAPL.parquet')\n",
    "\n",
    "df1, df2 = intersect_df(df1, df2, interpolate_to_days=True, extend_trend_to_today=False) # extend_trend_to_today should only be used when the macro data is recent.\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = get_emotion_df()\n",
    "x = np.array([x for x in emotion_df['text'].values])\n",
    "y = emotion_df[[x for x in emotion_df.columns if x != 'text']].values.astype('float32')\n",
    "x_shape = x[0].shape\n",
    "label_shape = y[0].shape\n",
    "x.shape, y.shape\n",
    "del emotion_df\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "#bs = int(bs * available_mem() // 8) # very conservative estimate of batch size based on available GPU memory\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "triplet_train = create_triplets(x_train, y_train, batch_size=bs, shuffle=True, seed=seed)\n",
    "triplet_test = create_triplets(x_test, y_test, batch_size=int(bs * 0.2), shuffle=True, seed=seed)\n",
    "del x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Given that the text emotion dataset is highly imbalanced, we will construct a siamese model to create an embedding that is seperable between the ~30 classes of emotions. This will give the classes equal probbaility of being accessed as well as serve as N^3 dataset augmentation. We can also easily compare the accuracy of the siamese network by adding a different head to the model and training it on a simpler ubalanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_siamese, model_encoder, model_inference = siamese_model(x_shape, label_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape, label_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback function\n",
    "try:\n",
    "    class CustomCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            # Log the validation loss at the end of each epoch\n",
    "            val_triplet_loss = logs['val_triplet_loss']\n",
    "            val_loss = logs['val_triplet_loss']\n",
    "\n",
    "    best_p = None\n",
    "    tuner = kt.Hyperband(siamese_model_dense,\n",
    "                            objective=kt.Objective(\"val_triplet_loss\", direction=\"min\"),\n",
    "                            hyperband_iterations=5,\n",
    "                            seed=seed,\n",
    "                            max_epochs=5,\n",
    "                            executions_per_trial=1,\n",
    "                            factor=3,\n",
    "                            directory='tuning',\n",
    "                            max_consecutive_failed_trials=10,\n",
    "                            project_name='siamese_emotion')\n",
    "\n",
    "\n",
    "    tuner.search(triplet_train.generator(),\n",
    "                epochs=5,\n",
    "                validation_data=triplet_test.generator(),\n",
    "                initial_epoch=1,\n",
    "                steps_per_epoch=triplet_train.num_batches//2,\n",
    "                validation_steps=triplet_test.num_batches,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(patience=0, monitor='val_triplet_loss'), \n",
    "                    CustomCallback()]\n",
    "                )\n",
    "\n",
    "    best_p = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Make sure the tuner cleanly exits if it is canceled by keyboard interrupt. The printed values should remain intact.\n",
    "# If fewer than 4 trials have been run, assume the tuner settings/model needs to be altered and delete the tuning directory.\n",
    "except KeyboardInterrupt as e:\n",
    "    if len(glob('tuning/*')) < 4:\n",
    "        rmtree('tuning/', ignore_errors=True)\n",
    "        rmtree('__pycache__/', ignore_errors=True)\n",
    "    print('Tuning canceled by user.')\n",
    "\n",
    "except Exception as e:\n",
    "    if len(glob('tuning/*')) < 4:\n",
    "        rmtree('tuning/', ignore_errors=True)\n",
    "        rmtree('__pycache__/', ignore_errors=True)\n",
    "    print('Tuning failed.')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(triplet_train, epochs=10, validation_data=triplet_test, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_siamese = model_siamese.fit(triplet_train.generator(), steps_per_epoch=triplet_train.num_batches, epochs=100, validation_data=triplet_test.generator(), validation_steps=triplet_test.num_batches, callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "# # freeze the layers\n",
    "# for layer in model_siamese.layers:\n",
    "#     layer.trainable = False\n",
    "# fig = plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history_siamese.history['loss'], label='train')\n",
    "# plt.plot(history_siamese.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_inference.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def download_file(url, filename, move_to=None):\n",
    "    \"\"\"Downloads a file from a url and saves it to the specified filename.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The url to download the file from.\n",
    "        filename (str): The filename to save the file to.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(move_to + filename) and not os.path.exists(move_to + filename.replace(filename.split('.')[-1], 'parquet')):\n",
    "        # create the directory if it doesn't exist\n",
    "        \n",
    "        with urllib.request.urlopen(url) as response, open(filename, 'wb') as out_file:\n",
    "            data = response.read() # a `bytes` object\n",
    "            out_file.write(data)\n",
    "\n",
    "    if move_to is not None:\n",
    "        if not os.path.exists(os.path.dirname(move_to)):\n",
    "            os.makedirs(os.path.dirname(move_to), exist_ok=True)\n",
    "        if not os.path.exists(move_to + filename) and not os.path.exists(move_to + filename.replace(filename.split('.')[-1], 'parquet')):\n",
    "            shutil.move(filename, move_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_emotion_df():\n",
    "    \"\"\"Parses the emotion dataframes and returns a dataframe with the emotion data that has been tokenized using of of the fasttext larger english cbow models.\n",
    "    \n",
    "    This saves a little bit of the processing time at the expense of storage space.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame\"\"\"\n",
    "    if os.path.exists('data/Emotions/emotion_df.parquet'):\n",
    "        emotion_df = load_file('data/Emotions/emotion_df.parquet')\n",
    "    else:\n",
    "        emotion_df = parse_emotion_dataframes([0, 1, 2, 3, 4], ensure_only_one_label=True)\n",
    "        #drop duplicates\n",
    "        emotion_df = emotion_df.drop_duplicates(subset=['text'])\n",
    "        if not os.path.exists('data/Emotions/crawl-300d-2M-subword.bin'):\n",
    "            #fasttext.util.download_model('en', if_exists='ignore')\n",
    "            \n",
    "            # download file https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
    "            download_file('https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip', 'crawl-300d-2M-subword.zip')\n",
    "            # unzip the file\n",
    "\n",
    "            with zipfile.ZipFile('crawl-300d-2M-subword.zip', 'r') as zip_ref:\n",
    "                zip_ref.extract('crawl-300d-2M-subword.bin')\n",
    "            # rename the file\n",
    "\n",
    "            os.rename('crawl-300d-2M-subword.bin', 'data/Emotions/crawl-300d-2M-subword.bin')\n",
    "            os.remove('crawl-300d-2M-subword.zip')\n",
    "            fast_text_model = fasttext.load_model('data/Emotions/crawl-300d-2M-subword.bin')\n",
    "        else:\n",
    "            fast_text_model = fasttext.load_model('data/Emotions/crawl-300d-2M-subword.bin')\n",
    "        \n",
    "        list_of_emotions = emotion_df.columns[1:]\n",
    "        \n",
    "        \n",
    "        # preprocess the text data using the fasttext model\n",
    "        emotion_df['text'] = emotion_df['text'].apply(lambda x: fast_text_model.get_sentence_vector(x))\n",
    "        # save the preprocessed data to a file as a parquet file\n",
    "        #save_file(emotion_df, 'data/Emotions/emotion_df.parquet')\n",
    "    emotion_df.dropna(inplace=True)\n",
    "    return emotion_df\n",
    "\n",
    "emotion_df = get_emotion_df()\n",
    "emotion_df.text[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = parse_emotion_dataframes([0, 1, 2, 3, 4], ensure_only_one_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts of each hot encoded emotion\n",
    "emotion_df_counts = emotion_df[emotion_df.columns[1:]].sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "words = []\n",
    "for word in emotion_df.text:\n",
    "    words.extend(word.split())\n",
    "words = list(set(words))\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "# find the form of a word using wordnet\n",
    "def get_wordnet_pos(word):\n",
    "\n",
    "    pos = wn.synsets(word)\n",
    "    if pos.__len__() > 0:\n",
    "       pos = pos[0].pos()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_df = pd.DataFrame(words, columns=['word'])\n",
    "\n",
    "\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "# Check whether a word is an emoji. If it is, then return a one word description of the emoji\n",
    "def convert_emojis(s):\n",
    "    return emoji.demojize(s)\n",
    "\n",
    "words_df['word'] = words_df['word'].apply(lambda x: convert_emojis(x))\n",
    "def replace_punctuation(s):\n",
    "    \"\"\"Replace puntuation with spaces\"\"\"\n",
    "    list_of_punctuation = string.punctuation\n",
    "\n",
    "    for p in list_of_punctuation:\n",
    "        s = s.replace(p, ' ')\n",
    "    return s.lower()\n",
    "\n",
    "words_df['word'] = words_df['word'].apply(lambda x: replace_punctuation(x))\n",
    "\n",
    "#if there are rows with spaces between words, then split them into separate rows\n",
    "words_df_split = words_df[words_df['word'].str.contains(' ')].copy()\n",
    "words_df = words_df[~words_df['word'].str.contains(' ')]\n",
    "words_df_split['word'] = words_df_split['word'].apply(lambda x: x.split(' '))\n",
    "words_df_split = words_df_split.explode('word')\n",
    "words_df = pd.concat([words_df, words_df_split], axis=0)\n",
    "\n",
    "# remove duplicates\n",
    "words_df = words_df.drop_duplicates(subset=['word'])\n",
    "# remove words that are just numbers\n",
    "words_df = words_df[~words_df['word'].str.isnumeric()]\n",
    "words_df['pos'] = words_df['word'].apply(lambda x: get_wordnet_pos(x))\n",
    "words_df = words_df[words_df['pos']!='']\n",
    "words_df = words_df[words_df['word'].apply(lambda x: x.__len__()>4)]\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all values of d\n",
    "words_df.pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word, pos):\n",
    "    synonyms = set()\n",
    "    for syn in wn.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "words_df['synonyms'] = words_df.apply(lambda x: get_synonyms(word=x['word'], pos=x['pos']), axis=1)\n",
    "# if synonyms is empty, then remove the row\n",
    "words_df = words_df[words_df['synonyms'].apply(lambda x: x.__len__()>0)]\n",
    "\n",
    "# if len synonyms is 1 and the synonym is the same as the word, then remove the row\n",
    "words_df = words_df[~((words_df['synonyms'].apply(lambda x: x.__len__()==1)) & (words_df['synonyms'].apply(lambda x: x[0])==words_df['word']))]\n",
    "\n",
    "words_df = words_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspellchecker \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def get_misspellings(file_path):\n",
    "    \n",
    "    misspellings = load_file(file_path)\n",
    "    misspellings = np.squeeze(misspellings.values).tolist()\n",
    "    if \":\" in misspellings[0]:\n",
    "        misspellings_dict = {key: value_split_list.replace(',', '').replace('\\n', '').split(' ') for key, value_split_list in [x.split(':') for x in misspellings]}\n",
    "        for key, value in misspellings_dict.items():\n",
    "            misspellings_dict[key] = [x for x in value if x not in [key, '', None] and \"*\" not in x]\n",
    "    elif \"$\" in \"\".join(misspellings[0:100]):\n",
    "        misspellings_list = [[] for x in range(0, misspellings.__len__())]\n",
    "        #misspellings = [x.replace('\\n', '') for x in misspellings]\n",
    "        spellings_list = []\n",
    "        for spelling_id in range(len(misspellings)):\n",
    "            word = misspellings[spelling_id]\n",
    "            if word != None:\n",
    "                \n",
    "                if \"$\" in word: \n",
    "                    spellings_list.append(word.replace(\"$\", \"\"))\n",
    "                else:\n",
    "                    misspellings_list[spelling_id].append(word)\n",
    "\n",
    "        misspellings_dict = {}\n",
    "        last_key = \"\"\n",
    "        \n",
    "        for i in range(len(spellings_list)):\n",
    "            if \"$\" in misspellings[i]:\n",
    "                last_key = misspellings[i].replace(\"$\", \"\")\n",
    "                misspellings_dict[last_key] = []\n",
    "            else:\n",
    "                if last_key != \"\":\n",
    "                    misspellings_dict[last_key].append([misspellings[i]])\n",
    "        for key, value in misspellings_dict.items():\n",
    "            misspellings_dict[key] = [x[0] for x in value if x not in [key, '', None] and \"*\" not in x]\n",
    "    return misspellings_dict\n",
    "\n",
    "\n",
    "urls = ['https://www.dcs.bbk.ac.uk/~ROGER/missp.dat', 'https://www.dcs.bbk.ac.uk/~ROGER/aspell.dat', 'https://www.dcs.bbk.ac.uk/~ROGER/wikipedia.dat', 'https://norvig.com/ngrams/spell-errors.txt']\n",
    "for file in urls:\n",
    "        download_file(file, file.split('/')[-1], move_to='data/Misspellings/')\n",
    "convert_project_files_to_parquet()\n",
    "urls = [x.replace(x.split('.')[-1], 'parquet') for x in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dicts = [get_misspellings(\"data/Misspellings/\" + x.split('/')[-1]) for x in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_spelling_dictionaries(list_of_files_to_load):\n",
    "    spell = SpellChecker()\n",
    "    for file in list_of_files_to_load:\n",
    "        download_file(file, file.split('/')[-1], move_to='data/Misspellings/')\n",
    "    convert_project_files_to_parquet()\n",
    "    list_dicts = [get_misspellings(\"data/Misspellings/\" + x.split('/')[-1]) for x in list_of_files_to_load]\n",
    "    # set of all keys in n dictionaries\n",
    "    union_keys = list(set(list_dicts[0]).union(*list_dicts[1:]))\n",
    "    misspellings_dict = {}\n",
    "    for key in union_keys:\n",
    "        misspellings_dict[key] = []\n",
    "        for misspellings_dict_n in list_dicts:\n",
    "            if key in misspellings_dict_n:\n",
    "                misspellings_dict[key].extend(misspellings_dict_n[key])\n",
    "\n",
    "    for key in misspellings_dict:\n",
    "        misspellings_dict[key] = list(set(misspellings_dict[key]))\n",
    "    \n",
    "    index_to_remove = []\n",
    "    for key, value in misspellings_dict.items():\n",
    "        for word in value:\n",
    "            if len(spell.unknown([word])) == 0 or word == '':\n",
    "                index_to_remove.append((key, word))\n",
    "    \n",
    "    for key, word in index_to_remove:\n",
    "        misspellings_dict[key].remove(word)\n",
    "\n",
    "    # remove empty lists\n",
    "    for key in list(misspellings_dict):\n",
    "        if not misspellings_dict[key]:\n",
    "            del misspellings_dict[key]\n",
    "\n",
    "    return misspellings_dict\n",
    "\n",
    "\n",
    "\n",
    "misspellings_dict = load_spelling_dictionaries(urls)\n",
    "\n",
    "def get_spell_variants(word, misspellings_dict):\n",
    "    if word in misspellings_dict:\n",
    "        return misspellings_dict[word]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# collect all misspellings for each word (str) and synonym (list) in words_df\n",
    "words_df['misspellings'] = words_df['word'].apply(lambda x: get_spell_variants(x, misspellings_dict))\n",
    "words_df['synonym_misspellings'] = words_df['synonyms'].apply(lambda x: [get_spell_variants(y, misspellings_dict) for y in x])\n",
    "# concat list of lists into one list\n",
    "words_df['synonym_misspellings'] = words_df['synonym_misspellings'].apply(lambda x: [item for sublist in x for item in sublist])\n",
    "# combine misspellings and synonym_misspellings and remove duplicates\n",
    "words_df['misspellings'] = words_df.apply(lambda x: list(set(x['misspellings'] + x['synonym_misspellings'])), axis=1)\n",
    "words_df.drop(columns=['synonym_misspellings'], inplace=True)\n",
    "words_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
