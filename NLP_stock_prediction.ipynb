{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = True\n",
    "\n",
    "\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "    #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "    #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "    del password;\n",
    "    gc.collect();\n",
    "    get_datasets(kaggle_api_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for training the sentiment analysis model\n",
    "# ~2m 42s first run, ~25s after\n",
    "classes, train_triplets, test_triplets, x_train, y_train, x_test, y_test = prep_triplet_data(MODEL=f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n",
    "ds_train, ds_test = prep_tensor_ds( x_train, y_train, x_test, y_test)\n",
    "classes_len = len(classes)\n",
    "\n",
    "siamese_network_model = siamese_network(classes_len).to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will train slower than using a tranditional torch dataset, since the triplet dataloader does not preprocess the data in parallel.\n",
    "siamese_model, history = pre_train_using_siamese(train_triplets, test_triplets, siamese_network_model, epochs=2, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classify_single_input(siamese_network_model)\n",
    "model = model.to(cuda)\n",
    "model, history = train_emotion_classifier(model, ds_train, ds_test, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "directory = 'models'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "torch.save(model.state_dict(), f'{directory}/emotion_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tor --controlport 9051\n",
    "# run this asynchronusly on a seperate thread\n",
    "tor_pass = os.urandom(32).hex()\n",
    "os.system(f'tor --controlport 9051 --hash-password {tor_pass} &')#--controlpassword {tor_pass} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor_pass = '16:8B50BA4D04CE492760868EC56A14B3E6D56D53F45A8E3435B8B59944E3';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file to store the password\n",
    "#find user dir\n",
    "import os\n",
    "torrc = f\"{os.path.expanduser('~')}/AppData/Roaming/tor/torrc\"\n",
    "if not os.path.exists(torrc):\n",
    "    with open(torrc, 'w') as f:\n",
    "        f.write(tor_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'tor --controlport 9051 --tor.password \"568456845678\" &')#--controlpassword {tor_pass}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Experiments with web scraping. Unfinished, so ignore if it does not work #############################\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "import scrapy\n",
    "import stweet as st\n",
    "import requests\n",
    "from stweet.search_runner import SearchRunContext\n",
    "import tor_python_easy\n",
    "import io\n",
    "#arrow time\n",
    "# import urllib.request as urllib2\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "# import yaml\n",
    "# INSTALL DOCKER COMPOSE\n",
    "\n",
    "# if not os.path.exists('docker-compose.yml'):\n",
    "#     url = 'https://raw.githubusercontent.com/markowanga/tor-python-easy/main/docker-compose.yml'\n",
    "#     filename = 'docker-compose.yml'\n",
    "#     urllib2.urlretrieve(url, filename);\n",
    "#     with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#     config['services']['torproxy']['environment'][0]=f'PASSWORD={os.urandom(32).hex()}';\n",
    "#     #with open('docker-compose.yml', 'w') as file:\n",
    "#        # yaml.dump(config, file);\n",
    "\n",
    "# with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#         tor_pass = config['services']['torproxy']['environment'][0].split('=')[1]; \n",
    "\n",
    "from tor_python_easy.tor_control_port_client import TorControlPortClient\n",
    "from tor_python_easy.tor_socks_get_ip_client import TorSocksGetIpClient\n",
    "proxy_config = {'http': 'socks5://localhost:9050','https': 'socks5://localhost:9050',}\n",
    "ip_client = TorSocksGetIpClient(proxy_config)\n",
    "tor_control_port_client = TorControlPortClient(control_address='localhost', control_port=9051, control_password=tor_pass)\n",
    "\n",
    "def url_fetch(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text, article.title, article.publish_date, article.authors, article.summary, article.keywords\n",
    "\n",
    "def crawl_url(url):\n",
    "    page = newspaper.build(url, memoize_articles=False)\n",
    "    articles = page.articles\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = arrow.get('2021-01-01').datetime\n",
    "until = arrow.get('2021-02-01').datetime\n",
    "exact_words = None\n",
    "all_words = None\n",
    "any_word = '#apple'\n",
    "\n",
    "web_client = st.DefaultTwitterWebClientProvider.get_web_client_preconfigured_for_tor_proxy(\n",
    "    socks_proxy_url='socks5://localhost:9050',\n",
    "    control_host='localhost',\n",
    "    control_password=tor_pass,\n",
    "    control_port=9051,\n",
    ")\n",
    "\n",
    "\n",
    "collect1 = st.CollectorRawOutput()\n",
    "collect2 = st.CollectorRawOutput()\n",
    "context = SearchRunContext()\n",
    "\n",
    "#since = arrow.get(since).datetime\n",
    "#until = arrow.get(until).datetime\n",
    "\n",
    "tweets_task = st.SearchTweetsTask(since=since, until=until, any_word=any_word, exact_words=exact_words,all_words=all_words, tweets_limit=50, replies_filter=True)\n",
    "runner = st.TweetSearchRunner(search_tweets_task=tweets_task,tweet_raw_data_outputs=[collect1],user_raw_data_outputs=[collect2], web_client=web_client, search_run_context=context)\n",
    "\n",
    "runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.search_run_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = collect1.get_raw_list()\n",
    "\n",
    "l2 = collect2.get_raw_list()\n",
    "\n",
    "len(l), len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = [value for key,value in json.loads(l[0].to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']]\n",
    "d[0] = arrow.get(d[0], 'ddd MMM DD HH:mm:ss Z YYYY').datetime.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set([[value for key,value in json.loads(x.to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']][1] for x in l])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
