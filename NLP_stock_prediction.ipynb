{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = False\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "MODEL = 'cardiffnlp/twitter-xlm-roberta-base-sentiment' #cardiffnlp/twitter-xlm-roberta-base-sentiment  cardiffnlp/twitter-roberta-base-emotion\n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    if len(password) > 0:\n",
    "        kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "        #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "        #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "        del password;\n",
    "        gc.collect();\n",
    "        get_datasets(kaggle_api_key);\n",
    "    elif len(glob('data/*')) < 4:\n",
    "        cont = input(f\"Password is empty. Press n to cancel or any other key to continue: \")\n",
    "        if cont == 'n':\n",
    "            assert False, \"Exiting program. Please enter a password to continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_stock_data(ticker='KR', save = True)\n",
    "update_stock_data(ticker='AAPL', save = True)\n",
    "update_stock_data(ticker='TSLA', save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'KR'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'AAPL'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "\n",
    "    # search = search_terms.data[search_terms.data['ticker'] == 'TSLA'].values.tolist()[0][0:4]\n",
    "    # search = [x.lower() for x in search if x != None and x != '']\n",
    "    # search[0] = '#' + search[0]\n",
    "    # df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "    \n",
    "\n",
    "# how to recover the data if the crawler crashes (example for apple inc.)\n",
    "#tweets_df = pd.DataFrame(df_if_error, columns=['date', 'text', 'username', 'searchterm'])\n",
    "#tweets_df = tweets_df.drop_duplicates(inplace=False, subset=['date', 'text', 'username', 'searchterm']).reset_index(drop=True, inplace=False).dropna(inplace=False)\n",
    "#save_file(tweets_df, r'data\\Twitter\\twitter_data___#aapl---apple inc.---timothy cook---consumer electronics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = emotion_classifier_load(MODEL)\n",
    "text_df = classify_twitter_text(save_path='data/Text/text_emotion_29.parquet', model=emotion_classifier, load_path='data/Text/text_emotion_29.parquet')\n",
    "text_df = text_df.drop(columns=['stock','text'], inplace=False).dropna(inplace=False)\n",
    "# len date row chars > 7\n",
    "text_df = text_df[text_df['date'].str.len() > 7]\n",
    "text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = glob('data/Twitter/twitter_emotion___#*.parquet')\n",
    "index = {x.split(\"#\")[1].split(\"---\")[0]: x for x in index}\n",
    "\n",
    "kr_text =   load_file(index['kr'].replace('emotion', 'data'))\n",
    "aapl_text = load_file(index['aapl'].replace('emotion', 'data'))\n",
    "\n",
    "kr_df   = classify_twitter_text(save_path=index['kr'], model=emotion_classifier, load_path=index['kr'].replace('emotion', 'data'))\n",
    "aapl_df = classify_twitter_text(save_path=index['aapl'], model=emotion_classifier, load_path=index['aapl'].replace('emotion', 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kr stock\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "#kr_df, stock_kr = intersect_df(kr_df, stock_kr)\n",
    "kr_df['date'] = kr_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "#kr_df, stock_kr = intersect_df(kr_df, stock_kr)\n",
    "kr_stats = daily_stats(kr_df)\n",
    "\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "stock_kr, text_df_kr = intersect_df(stock_kr, text_df)\n",
    "text_stats_kr = daily_stats(text_df_kr)\n",
    "text_df_kr = daily_stats(text_df_kr)\n",
    "\n",
    "\n",
    "\n",
    "# aapl stock\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "#aapl_df, stock_aapl = intersect_df(aapl_df, stock_aapl)\n",
    "aapl_df['date'] = aapl_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "#aapl_df, stock_aapl = intersect_df(aapl_df, stock_aapl)\n",
    "aapl_stats = daily_stats(aapl_df)\n",
    "\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "stock_aapl, text_df_appl = intersect_df(stock_aapl, text_df)\n",
    "text_stats_appl = daily_stats(text_df_appl)\n",
    "text_df_appl = daily_stats(text_df_appl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter to emotion dataframe\n",
    "\n",
    "feature list: \n",
    "\n",
    "              emotions        [std, mean, count],                                 --> no need to normalize\n",
    "              \n",
    "              stock           [open, high, low, close, volume],                   --> normalize percent change\n",
    "\n",
    "              portfolio       [current money, stocks owned, stocks owned value],  --> keep track of portfolio\n",
    "\n",
    "              date            [day of week, holiday, day of month, month]         --> engineer\n",
    "\n",
    "              economic data   [NA]                                                --> normalize \n",
    "\n",
    "output: \n",
    "\n",
    "  volatility:\n",
    "\n",
    "  best action\n",
    "\n",
    "  predicted price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_timeline(company_emotion_stats, stock, generic_news_stats, fillzero=True, impute=True):\n",
    "    \"\"\"\n",
    "    Merges the data from the company emotion stats and the generic news stats into a single dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    company_emotion_stats : pandas.DataFrame\n",
    "        The dataframe containing the company emotion stats\n",
    "    generic_news_stats : pandas.DataFrame\n",
    "        The dataframe containing the generic news stats\n",
    "    fillzero : bool, optional\n",
    "        Whether to fill the NaN values with 0, by default True\n",
    "    impute : bool, optional\n",
    "        Whether to impute the NaN values (missing days), by default True\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The merged dataframe by date, with zeros for all values where there is no intersection \n",
    "    \"\"\"\n",
    "    company_emotion_stats.date = company_emotion_stats.date.apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None)\n",
    "    generic_news_stats.date = generic_news_stats.date.apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None)\n",
    "    df = pd.merge(company_emotion_stats, generic_news_stats, how='outer', on='date')\n",
    "    df = df.sort_values(by='date', inplace=False)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    # add one day to stock data so we can have the next day's as the target\n",
    "    #stock['date'] = stock['date'].apply(lambda x: arrow.get(x).shift(days=1).format('YYYY-MM-DD') if arrow.get(x).weekday() < 5 else arrow.get(x).shift(days=2).format('YYYY-MM-DD') if arrow.get(x).weekday() == 5 else arrow.get(x).shift(days=3).format('YYYY-MM-DD'))\n",
    "\n",
    "    df, stock = intersect_df(df, stock)\n",
    "    df = pd.merge(df, stock, how='outer', on='date')\n",
    "    df = df.sort_values(by='date', inplace=False).reset_index(drop=True, inplace=False)\n",
    "    df['day_of_week'] = df['date'].apply(lambda x: arrow.get(x).weekday())\n",
    "    df['day_of_month'] = df['date'].apply(lambda x: arrow.get(x).day)\n",
    "    df['month'] = df['date'].apply(lambda x: arrow.get(x).month)\n",
    "    df['mon_or_fri'] = df['day_of_week'].apply(lambda x: 1 if x == 0 or x == 4 else 0)\n",
    "    # percent change\n",
    "    df['open'] = df['open'].pct_change()\n",
    "    df['high'] = df['high'].pct_change()\n",
    "    df['low'] = df['low'].pct_change()\n",
    "    df['volume'] = df['volume'].pct_change()\n",
    "    df['close'] = df['close'].pct_change()\n",
    "    if fillzero:\n",
    "        df = df.fillna(0)\n",
    "    if impute:\n",
    "        df = df.interpolate(method='linear')\n",
    "    \n",
    "    return  df\n",
    "\n",
    "merged_df = merge_data_timeline(kr_stats, stock_kr, text_stats_kr)\n",
    "merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "x_cols = merged_df.columns #  if x not in y_cols and x != 'date'... not removed because it is current day's data and we want to predict next day's data\n",
    "X = merged_df[x_cols]\n",
    "Y = merged_df[y_cols]\n",
    "#delete the first row to allign the data\n",
    "Y = Y.drop(Y.index[0]) # y is now the next day's data... e.g. we want to predict tomorrow's stock price\n",
    "X = X.drop(X.index[-1]) \n",
    "\n",
    "# complete timeseries\n",
    "Y = Y.values\n",
    "X = X.values\n",
    "\n",
    "# split into train and test sets by slicing\n",
    "train_size = int(len(X) * 0.90)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, X_test = X[0:train_size,:], X[train_size:len(X),:]\n",
    "Y_train, Y_test = Y[0:train_size,:], Y[train_size:len(Y),:]\n",
    "\n",
    "dates_train = X_train[:,0]\n",
    "dates_test = X_test[:,0]\n",
    "X_train = X_train[:,1:].astype('float32')\n",
    "X_test = X_test[:,1:].astype('float32')\n",
    "\n",
    "# convert to tensor\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "\n",
    "Y_train.shape, X_train.shape, Y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinConfig, SwinModel\n",
    "configuration = SwinConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class portfolio():\n",
    "    def __init__(self, funds):\n",
    "        self.funds = funds\n",
    "        self.stocks_owned = {}\n",
    "\n",
    "    def de_norm_loss(self):\n",
    "        \"\"\"undoes the operations performed to normalize the data\"\"\"\n",
    "        pass\n",
    "\n",
    "    def loss_action(self, y_action, y_reality):\n",
    "        \"\"\"calculates how well the portfolio is doing\"\"\"\n",
    "        pass\n",
    "\n",
    "    def loss_innacuracy(self, y_pred, y_reality):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
