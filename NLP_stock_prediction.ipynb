{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. \n",
    "\n",
    "        Since this notebook takes a tremendous ammount of data to run, it is not included in the repo. All the necessary steps to aquiring the data are included in the notebook however, although the twitter scraper might become broken as twitter changes their API to block scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = False\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "MODEL = 'cardiffnlp/twitter-xlm-roberta-base-sentiment' #cardiffnlp/twitter-xlm-roberta-base-sentiment  cardiffnlp/twitter-roberta-base-emotion\n",
    "print('Available GPU memory:', available_mem(), 'GB')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    if len(password) > 0:\n",
    "        kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "        #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "        #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "        del password;\n",
    "        gc.collect();\n",
    "        get_datasets(kaggle_api_key);\n",
    "    elif len(glob('data/*')) < 4:\n",
    "        cont = input(f\"Password is empty. Press n to cancel or any other key to continue: \")\n",
    "        if cont == 'n':\n",
    "            assert False, \"Exiting program. Please enter a password to continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_stock_data(ticker='KR', save = True)\n",
    "update_stock_data(ticker='AAPL', save = True)\n",
    "update_stock_data(ticker='TSLA', save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are used to scrape tweets from twitter. However, it has recently stopped working due to aggressive rate limiting/anti-bot measures.\n",
    "\n",
    "if False:\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'KR'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'AAPL'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "\n",
    "    # search = search_terms.data[search_terms.data['ticker'] == 'TSLA'].values.tolist()[0][0:4]\n",
    "    # search = [x.lower() for x in search if x != None and x != '']\n",
    "    # search[0] = '#' + search[0]\n",
    "    # df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "    \n",
    "\n",
    "# how to recover the data if the crawler crashes (example for apple inc.)\n",
    "#tweets_df = pd.DataFrame(df_if_error, columns=['date', 'text', 'username', 'searchterm'])\n",
    "#tweets_df = tweets_df.drop_duplicates(inplace=False, subset=['date', 'text', 'username', 'searchterm']).reset_index(drop=True, inplace=False).dropna(inplace=False)\n",
    "#save_file(tweets_df, r'data\\Twitter\\twitter_data___#APPL---apple inc.---timothy cook---consumer electronics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = emotion_classifier_load(MODEL)\n",
    "text_df = classify_twitter_text(save_path='data/Text/text_emotion_29.parquet', model=emotion_classifier, load_path='data/Text/text_emotion_29.parquet')\n",
    "text_df = text_df.drop(columns=['stock','text'], inplace=False).dropna(inplace=False)\n",
    "# len date row chars > 7\n",
    "text_df = text_df[text_df['date'].str.len() > 7]\n",
    "text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = glob('data/Twitter/twitter_emotion___#*.parquet')\n",
    "index = {x.split(\"#\")[1].split(\"---\")[0]: x for x in index}\n",
    "\n",
    "kr_text =   load_file(index['kr'].replace('emotion', 'data'))\n",
    "aapl_text = load_file(index['aapl'].replace('emotion', 'data'))\n",
    "\n",
    "kr_df   = classify_twitter_text(save_path=index['kr'], model=emotion_classifier, load_path=index['kr'].replace('emotion', 'data'))\n",
    "aapl_df = classify_twitter_text(save_path=index['aapl'], model=emotion_classifier, load_path=index['aapl'].replace('emotion', 'data'))\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calls 1) load the stock data, 2) intersect the stock data with the text data, and 3) calculate the daily stats for the stock and text data. Daily stats converts the 20 or so tweets per day into a single row of data with the mean, std, and other stats for the day.\n",
    "\n",
    "# kr stock\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "kr_df['date'] = kr_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "kr_stats = daily_stats(kr_df)\n",
    "\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "stock_kr, text_df_kr = intersect_df(stock_kr, text_df)\n",
    "text_stats_kr = daily_stats(text_df_kr)\n",
    "text_df_kr = daily_stats(text_df_kr)\n",
    "\n",
    "\n",
    "\n",
    "# aapl stock\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "aapl_df['date'] = aapl_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "aapl_stats = daily_stats(aapl_df)\n",
    "\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "stock_aapl, text_df_appl = intersect_df(stock_aapl, text_df)\n",
    "text_stats_appl = daily_stats(text_df_appl)\n",
    "text_df_appl = daily_stats(text_df_appl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter to emotion dataframe\n",
    "\n",
    "feature list: \n",
    "\n",
    "              emotions        [std, mean, count],                                 --> no need to normalize\n",
    "              \n",
    "              stock           [open, high, low, close, volume],                   --> normalize percent change\n",
    "\n",
    "              portfolio       [current money, stocks owned, stocks owned value],  --> keep track of portfolio\n",
    "\n",
    "              date            [day of week, holiday, day of month, month]         --> engineer\n",
    "\n",
    "              economic data   [NA]                                                --> normalize \n",
    "\n",
    "output: \n",
    "\n",
    "  volatility:\n",
    "\n",
    "  best action\n",
    "\n",
    "  predicted price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process kroger data. 1) Merge global, stock, and company specific data into a timeseries, with zeros for missing data. 2) Split into train and test sets. 3) Scale the data. 4) Reshape the data into 3D for Resnet, 5) ensure data ordering and labels are correct\n",
    "\n",
    "merged_df_kr = merge_data_timeline(kr_stats, stock_kr, text_stats_kr)\n",
    "y_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "x_cols = merged_df_kr.columns #  if x not in y_cols and x != 'date'... not removed because it is current day's data and we want to predict next day's data\n",
    "X = merged_df_kr[x_cols]\n",
    "Y = merged_df_kr[y_cols]\n",
    "#delete the first row to allign the data\n",
    "Y = Y.drop(Y.index[0]) # y is now the next day's data... e.g. we want to predict tomorrow's stock price\n",
    "X = X.drop(X.index[-1]) \n",
    "\n",
    "# complete timeseries\n",
    "Y = Y.values\n",
    "X = X.values\n",
    "\n",
    "# split into train and test sets by slicing\n",
    "train_size = int(len(X) * 0.90)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, Y_train_kr = X[0:train_size,:], Y[0:train_size,:]\n",
    "X_test, Y_test_kr = X[train_size:len(X),:], Y[train_size:len(Y),:]\n",
    "\n",
    "dates_train = X_train[:,0]\n",
    "dates_test = X_test[:,0]\n",
    "X_train = X_train[:,1:].astype('float32')\n",
    "X_test = X_test[:,1:].astype('float32')\n",
    "\n",
    "# split dim 1 into 2\n",
    "split_into = 2\n",
    "X_train_kr = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_kr = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "#################################################################################################################################\n",
    "# process apple data\n",
    "merged_df_aapl = merge_data_timeline(aapl_stats, stock_aapl, text_stats_appl)\n",
    "y_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "x_cols = merged_df_kr.columns #  if x not in y_cols and x != 'date'... not removed because it is current day's data and we want to predict next day's data\n",
    "X = merged_df_aapl[x_cols]\n",
    "Y = merged_df_aapl[y_cols]\n",
    "#delete the first row to allign the data\n",
    "Y = Y.drop(Y.index[0]) # y is now the next day's data... e.g. we want to predict tomorrow's stock price\n",
    "X = X.drop(X.index[-1]) \n",
    "\n",
    "# complete timeseries\n",
    "Y = Y.values\n",
    "X = X.values\n",
    "\n",
    "# split into train and test sets by slicing\n",
    "train_size = int(len(X) * 0.90)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, Y_train_aapl = X[0:train_size,:], Y[0:train_size,:]\n",
    "X_test, Y_test_aapl = X[train_size:len(X),:], Y[train_size:len(Y),:]\n",
    "\n",
    "dates_train = X_train[:,0]\n",
    "dates_test = X_test[:,0]\n",
    "X_train = X_train[:,1:].astype('float32')\n",
    "X_test = X_test[:,1:].astype('float32')\n",
    "\n",
    "# split dim 1 into 2\n",
    "split_into = 2\n",
    "X_train_aapl = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_aapl = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioTracker():\n",
    "    def __init__(self, cash=1000, ticker='KR'):\n",
    "        self.portfolio_history = {'cash': [], 'stocks': [], 'total': [], 'action': []}\n",
    "        self.cash = cash\n",
    "        self.start_cash = torch.as_tensor(10000, dtype=torch.float32)\n",
    "        self.price = torch.as_tensor(0, dtype=torch.float32)\n",
    "        self.stocks = torch.as_tensor(0, dtype=torch.float32)\n",
    "        self.stock_ticker_symbol = ticker\n",
    "        self.i_train = 0\n",
    "        self.i_test = 0\n",
    "        self.get_stock_history(ticker)\n",
    "\n",
    "    def action(self, amount, train=True, track=False):\n",
    "        \"\"\"actions are either 0, 1, or -1\"\"\"\n",
    "       \n",
    "        amount = amount.squeeze(0).item()\n",
    "        if train: \n",
    "            price = torch.as_tensor(self.train['close'][self.i_train], dtype=torch.float32)\n",
    "            self.i_train += 1\n",
    "            if self.i_train >= len(self.train): self.i_train = 0\n",
    "        else: \n",
    "            price = torch.as_tensor(self.test['close'][self.i_test], dtype=torch.float32)\n",
    "            self.i_test += 1\n",
    "            if self.i_test >= len(self.test): self.i_test = 0\n",
    "\n",
    "        self.price = price\n",
    "        price.requires_grad = True\n",
    "        old_value = self.get_value(price)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if amount > 0.33:\n",
    "                if self.cash >= price:\n",
    "                    self.cash = torch.subtract(self.cash, price)\n",
    "                    self.stocks = torch.add(self.stocks, 1.0)\n",
    "                else:\n",
    "                    pass\n",
    "            elif amount < -0.33:\n",
    "                if self.stocks >= 1.0:\n",
    "                    self.cash = torch.add(self.cash, price)\n",
    "                    self.stocks = torch.subtract(self.stocks, 1.0)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        new_value = self.get_value(price)\n",
    "\n",
    "        # maximize profit (reward)\n",
    "        change =  old_value - new_value\n",
    "        diff_from_start = self.start_cash - new_value\n",
    "        \n",
    "        if track:\n",
    "            self.portfolio_history['cash'].append(self.cash.detach().numpy())\n",
    "            self.portfolio_history['stocks'].append(self.stocks.detach().numpy())\n",
    "            self.portfolio_history['total'].append(self.cash.detach().numpy() + self.stocks.cpu().detach().numpy() * price.detach().numpy())\n",
    "            if amount > 0.33:\n",
    "                if self.cash >= price:\n",
    "                    self.portfolio_history['action'].append('buy')\n",
    "                else:\n",
    "                    self.portfolio_history['action'].append('hold')\n",
    "            elif amount < -0.33:\n",
    "                if self.stocks >= 1.0:\n",
    "                    self.portfolio_history['action'].append('sell')\n",
    "                else:\n",
    "                    self.portfolio_history['action'].append('hold')\n",
    "            else:\n",
    "                self.portfolio_history['action'].append('hold')\n",
    "        \n",
    "        loss1 = torch.as_tensor(change, dtype=torch.float32).to('cuda')\n",
    "        loss2 = torch.as_tensor(diff_from_start, dtype=torch.float32).to('cuda')\n",
    "        loss = torch.max(loss1+loss2 + 100, torch.as_tensor(0, dtype=torch.float32).to('cuda'))\n",
    "        return loss\n",
    "        \n",
    "    def get_value(self, price):\n",
    "        return torch.add(self.cash,  torch.multiply(self.stocks, price))\n",
    "\n",
    "    def get_portfolio_tensor(self):\n",
    "        p1 = torch.as_tensor(self.cash, dtype=torch.float32).cpu().reshape(1,)\n",
    "        p2 = torch.as_tensor(self.stocks, dtype=torch.float32).cpu().reshape(1,)\n",
    "        p3 = torch.as_tensor(self.price, dtype=torch.float32).cpu().reshape(1,)\n",
    "        portfolio_data = torch.concat((p1, p2, p3), 0)\n",
    "        p = portfolio_data.to('cuda')\n",
    "        return p\n",
    "\n",
    "    def get_stock_history(self, ticker):\n",
    "        self.train = load_file(f'data/Stock/{ticker}.parquet')\n",
    "        self.test = load_file(f'data/Stock/{ticker}.parquet')\n",
    "\n",
    "class StockModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(StockModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.resnet = models.resnet18(pretrained=False)\n",
    "        self.new_input_size = input_size\n",
    "        # update resnet input size\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.conv1.weight = nn.Parameter(self.resnet.conv1.weight.sum(dim=1, keepdim=True))\n",
    "        self.resnet.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.resnet.fc = nn.LazyLinear(self.num_classes)\n",
    "        # add head for RL agent\n",
    "        self.rl_head = nn.LazyLinear(1)\n",
    "        self.dense_portfolio = nn.LazyLinear(5)\n",
    "        self.dense_resnet = nn.LazyLinear(5)\n",
    "        self.dense = nn.LazyLinear(5)\n",
    "\n",
    "    def forward(self, x, portfolio):\n",
    "        \"\"\"forward pass. Here we take the resnet model and add a head for the RL agent to learn from. This forward pass will then output the action taken and the pred_price for price.\n",
    "        \n",
    "        \"\"\"\n",
    "        # regular resnet forward pass\n",
    "        pred_price = self.resnet(x)\n",
    "        \n",
    "        # rl head forward pass\n",
    "        dense_portfolio = self.dense_portfolio(portfolio)\n",
    "        out1 = torch.squeeze(pred_price, 0)\n",
    "        dense_portfolio = torch.squeeze(dense_portfolio, 0)\n",
    "        dense_resnet = self.dense_resnet(out1)\n",
    "        dense = torch.cat((dense_portfolio, dense_resnet))\n",
    "        outcat2 = torch.concat((dense, out1))\n",
    "        action = self.rl_head(outcat2)\n",
    "\n",
    "        # \n",
    "        pred_price = torch.functional.F.softmax(pred_price, dim=1)\n",
    "        return pred_price, action.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_Size = 32\n",
    "X_train_win_kr = sliding_window_view_generator(X_train_kr, window_Size)\n",
    "X_test_win_kr = sliding_window_view_generator(X_test_kr, window_Size)\n",
    "Y_train_win_kr = sliding_window_view_generator(Y_train_kr, 1)\n",
    "Y_test_win_kr = sliding_window_view_generator(Y_test_kr, 1)\n",
    "\n",
    "X_train_win_aapl = sliding_window_view_generator(X_train_aapl, window_Size)\n",
    "X_test_win_aapl = sliding_window_view_generator(X_test_aapl, window_Size)\n",
    "Y_train_win_aapl = sliding_window_view_generator(Y_train_aapl, 1)\n",
    "Y_test_win_aapl = sliding_window_view_generator(Y_test_aapl, 1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.to(device)\n",
    "model = StockModel((1, window_Size, 216), Y_test_win_kr.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# vars\n",
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_model_perams = None\n",
    "patience = 10\n",
    "hist = {'train_loss_kr': [], 'test_loss_kr': [], 'train_loss_aapl': [], 'test_loss_aapl': []}\n",
    "num_epochs = 100\n",
    "early_stopping = True\n",
    "\n",
    "\n",
    "# Train and test the model on both the KR and AAPL data\n",
    "try:\n",
    "    with tqdm(total=num_epochs * (len(X_train_win_kr.x) + len(X_test_win_kr.x) + len(X_train_win_aapl.x) + len(X_test_win_aapl.x))) as pbar:\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            X_train_win_kr.current_index = 0\n",
    "            Y_train_win_kr.current_index = 0\n",
    "            model.train()\n",
    "            portfolio_tracker = PortfolioTracker(torch.as_tensor(10000, dtype=torch.float32).unsqueeze(0), ticker = 'KR')\n",
    "            # Train the model on the KR  data\n",
    "            for i in range(len(X_train_win_kr.x)):\n",
    "                # Forward pass\n",
    "                #model.train()\n",
    "                x = next(X_train_win_kr.next())\n",
    "                y = next(Y_train_win_kr.next())\n",
    "                predictions, actions  = model.forward(x, portfolio_tracker.get_portfolio_tensor())\n",
    "                loss_action = portfolio_tracker.action(actions, train=True, track=False)\n",
    "                loss_predic = criterion(predictions, y)\n",
    "                loss = loss_action + loss_predic\n",
    "                loss = loss\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                hist['train_loss_kr'].append(loss.item())\n",
    "                avg_loss = np.mean(hist['train_loss_kr']) if len(hist['train_loss_kr']) > 0 else np.mean(hist['train_loss_kr'])\n",
    "                pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.16f}')\n",
    "                pbar.update(1)\n",
    "\n",
    "            portfolio_tracker = PortfolioTracker(torch.as_tensor(10000, dtype=torch.float32).unsqueeze(0), ticker = 'AAPL')\n",
    "            X_train_win_aapl.current_index = 0\n",
    "            Y_train_win_aapl.current_index = 0\n",
    "            model.train()\n",
    "            # Train the model on aapl data\n",
    "            for i in range(len(X_train_win_kr.x)):\n",
    "                # Forward pass\n",
    "                #model.train()\n",
    "                x = next(X_train_win_kr.next())\n",
    "                y = next(Y_train_win_kr.next())\n",
    "                predictions, actions  = model.forward(x, portfolio_tracker.get_portfolio_tensor())\n",
    "                #sleep(1000)\n",
    "                loss_action = portfolio_tracker.action(actions, train=True, track=False)\n",
    "                loss_predic = criterion(predictions, y)\n",
    "                loss = loss_action + loss_predic\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                hist['train_loss_aapl'].append(loss.item())\n",
    "                avg_loss = np.mean(hist['train_loss_aapl']) if len(hist['train_loss_aapl']) > 0 else np.mean(hist['train_loss_aapl'])\n",
    "                pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.16f}')\n",
    "                pbar.update(1)\n",
    "\n",
    "            model.eval()\n",
    "            portfolio_tracker = PortfolioTracker(torch.as_tensor(10000, dtype=torch.float32).unsqueeze(0), ticker = 'KR')\n",
    "            Y_test_win_kr.current_index = 0\n",
    "            X_test_win_kr.current_index = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(X_test_win_kr.x)//100):\n",
    "                    x = next(X_test_win_kr.next())\n",
    "                    y = next(Y_test_win_kr.next())\n",
    "                    predictions, actions = model.forward(x, portfolio_tracker.get_portfolio_tensor())\n",
    "                    #sleep(1000)\n",
    "                    loss_action = portfolio_tracker.action(actions, train=False, track=True)\n",
    "                    loss_predic = criterion(predictions, y)\n",
    "                    loss = loss_action + loss_predic\n",
    "                    hist['test_loss_kr'].append(loss.item())\n",
    "                    avg_loss = np.mean(hist['test_loss_kr']) if len(hist['test_loss_kr']) > 0 else np.mean(hist['test_loss_kr'])\n",
    "                    pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.16f}')\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if early_stopping:\n",
    "                    if avg_loss < best_loss:\n",
    "                        best_loss = avg_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_model_perams = model.state_dict()\n",
    "                    elif epoch - best_epoch > patience:\n",
    "                        print('Early stopping')\n",
    "                        model.load_state_dict(best_model_perams)\n",
    "                        break\n",
    "\n",
    "            model.eval()\n",
    "            portfolio_tracker = PortfolioTracker(torch.as_tensor(10000, dtype=torch.float32).unsqueeze(0), ticker = 'AAPL')\n",
    "            Y_test_win_aapl.current_index = 0\n",
    "            X_test_win_aapl.current_index = 0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(X_test_win_aapl.x)//100):\n",
    "                    x = next(X_test_win_aapl.next())\n",
    "                    y = next(Y_test_win_aapl.next())\n",
    "                    predictions, actions = model.forward(x, portfolio_tracker.get_portfolio_tensor())\n",
    "                    #sleep(1000)\n",
    "                    loss_action = portfolio_tracker.action(actions, train=False, track=True)\n",
    "                    loss_predic = criterion(predictions, y)\n",
    "                    loss = loss_action + loss_predic\n",
    "                    hist['test_loss_aapl'].append(loss.item())\n",
    "                    avg_loss = np.mean(hist['test_loss_aapl']) if len(hist['test_loss_aapl']) > 0 else np.mean(hist['test_loss_aapl'])\n",
    "                    pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.16f}')\n",
    "                    pbar.update(1)\n",
    "\n",
    "                if early_stopping:\n",
    "                    if avg_loss < best_loss:\n",
    "                        best_loss = avg_loss\n",
    "                        best_epoch = epoch\n",
    "                        best_model_perams = model.state_dict()\n",
    "                    elif epoch - best_epoch > patience:\n",
    "                        print('Early stopping')\n",
    "                        model.load_state_dict(best_model_perams)\n",
    "                        break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Training interrupted by user')\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
