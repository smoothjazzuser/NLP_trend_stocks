{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that the following packages are installed (will not check for versioning as long as everything is installed, else it will install the last Tensorflow version supported by native windows 11/10)\n",
    "from importlib.util import find_spec\n",
    "packages = \"compress_pickle[lz4] pandas numpy yahooquery cryptography seaborn kaggle pyarrow transformers fasttext tensorflow==2.10.1 keras-tuner scikit-learn tensorflow_datasets tensorflow-text lime\"\n",
    "\n",
    "not_installed = [package_name.split('[')[0].split('=')[0].replace('scikit-learn','sklearn').replace('-','_') for package_name in packages.split(\" \")] # remove versioning and other stuff\n",
    "not_installed = [package_name for package_name in not_installed if find_spec(package_name) is None] # check if package is installed\n",
    "\n",
    "if len(not_installed) > 0:\n",
    "    %pip install -U $packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from glob import glob\n",
    "from compress_pickle import dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from yahooquery import Ticker\n",
    "import timeit\n",
    "import time\n",
    "import gc, os\n",
    "import datetime\n",
    "from getpass import getpass\n",
    "from shutil import rmtree\n",
    "from utils import fernet_key_encryption, aquire_stock_search_terms as aquire_terms, get_macroeconomic_data as macro_data, download_datasets, save_file, load_file, interpolate_months_to_days, intersect_df, parse_emotion_dataframes, get_emotion_df, create_triplets\n",
    "from models import siamese_model, triplet_loss\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers, losses, metrics\n",
    "import keras_tuner as kt\n",
    "\n",
    "    \n",
    "\n",
    "# enable memory growth for GPU and mixed precision\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    import subprocess\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    vram = tf.config.experimental.get_memory_growth(physical_devices[0])\n",
    "    print('Memory growth:', vram)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')\n",
    "    print(physical_devices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_mem():\n",
    "    \"\"\"Return the available GPU memory in GB.\"\"\"\n",
    "    MB_memory = int(\"\".join([x for x in subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.free\", \"--format=csv\"]).decode() if x.isdigit()]))\n",
    "    GB_memory = MB_memory / 1000\n",
    "    return GB_memory\n",
    "\n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this first run, you will only need to enter the password to load the api keys.\n",
    "# If you need to change the keys or password, delete the relevent .secret keys file and run this section again.\n",
    "# salt.secret is a non-sensitive file that is used to both generate the encryption key as well as decryption. If this key is lost, the encrypted files are lost and you will need to re-enter the api keys.\n",
    "\n",
    "# Ask for input of password to save API keys\n",
    "password = getpass(\"Enter password to save/load API keys: \");\n",
    "\n",
    "kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "#td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "\n",
    "#data.nasdaq.com api key through quandle\n",
    "data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "\n",
    "del password;\n",
    "gc.collect();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this (or any other) section fails initially, you may need to run it again. Each of these steps caches results to disk to speed up future runs and save RAM usage (they only will run once).\n",
    "username, password = kaggle_api_key.split(' ');\n",
    "os.environ['KAGGLE_USERNAME'] = username;\n",
    "os.environ['KAGGLE_KEY'] = password;\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd();\n",
    "\n",
    "# download the various kaggle datasets\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/sarthmirashi07/us-macroeconomic-data', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'US_macroeconomics.csv': 'macro/US_macroeconomics.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Macro')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'us-stock-dataset/Data/Stocks': 'Stocks', 'us-stock-dataset/Stock_List.csv': 'Stock_List.csv'}, \n",
    "        delete=True,\n",
    "        dest_name='Stocks')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/mathurinache/goemotions',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'goemotions.csv': 'Emotions/goemotions.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/parulpandey/emotion-dataset',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'training.csv': 'Emotions/training.csv', 'validation.csv': 'Emotions/validation.csv', 'test.csv': 'Emotions/test.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/kosweet/cleaned-emotion-extraction-dataset-from-twitter',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'dataset(clean).csv': 'Emotions/dataset(clean).csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'raw_partner_headlines.csv': 'Text/raw_partner_headlines.csv', 'raw_analyst_ratings.csv': 'Text/raw_analyst_ratings.csv', 'analyst_ratings_processed.csv': 'Text/analyst_ratings_processed.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Text')\n",
    "\n",
    "\n",
    "\n",
    "# clear the username and key from the environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = \"\" \n",
    "os.environ['KAGGLE_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the company info for all the ticker symbols and return a dataframe with relevant search terms for each company.\n",
    "# If the stocks dataset is updated on kaggle, compank_list.pkl needs to be deleted and this run again if the symbols have changed. \n",
    "# TODO: It would be more efficient to manually pull the new stock data ourselves and keep the old ticker symbols.\n",
    "search_terms = aquire_terms('data/Stocks/')\n",
    "search_terms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_file('data/Macro/US_macroeconomics.parquet')\n",
    "df2 = load_file('data\\Stocks\\AAPL.parquet')\n",
    "\n",
    "df1, df2 = intersect_df(df1, df2, interpolate_to_days=True, extend_trend_to_today=False) # extend_trend_to_today should only be used when the macro data is recent.\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = get_emotion_df()\n",
    "x = np.array([x for x in emotion_df['text'].values])\n",
    "y = emotion_df[[x for x in emotion_df.columns if x != 'text']].values.astype('float32')\n",
    "x_shape = x[0].shape\n",
    "label_shape = y[0].shape\n",
    "x.shape, y.shape\n",
    "del emotion_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "#bs = int(bs * available_mem() // 8) # very conservative estimate of batch size based on available GPU memory\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "triplet_train = create_triplets(x_train, y_train, batch_size=bs, shuffle=True, seed=42)\n",
    "triplet_test = create_triplets(x_test, y_test, batch_size=int(bs * 0.2), shuffle=True, seed=42)\n",
    "del x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Given that the text emotion dataset is highly imbalanced, we will construct a siamese model to create an embedding that is seperable between the ~30 classes of emotions. This will give the classes equal probbaility of being accessed as well as serve as N^3 dataset augmentation. We can also easily compare the accuracy of the siamese network by adding a different head to the model and training it on a simpler ubalanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_siamese, model_encoder, model_inference = siamese_model(x_shape, label_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape, label_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(siamese_model,\n",
    "                        objective=kt.Objective(\"triplet_loss\", direction=\"min\"),\n",
    "                        hyperband_iterations=5,\n",
    "                        seed=42,\n",
    "                        factor=3,\n",
    "                        directory='tuning',\n",
    "                        project_name='siamese_emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(triplet_train,\n",
    "                epochs=10,\n",
    "                validation_data=triplet_test,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(triplet_train, epochs=10, validation_data=triplet_test, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_siamese = model_siamese.fit(triplet_train.generator(), steps_per_epoch=triplet_train.num_batches, epochs=100, validation_data=triplet_test.generator(), validation_steps=triplet_test.num_batches, callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "# # freeze the layers\n",
    "# for layer in model_siamese.layers:\n",
    "#     layer.trainable = False\n",
    "# fig = plt.figure(figsize=(10, 5))\n",
    "# plt.plot(history_siamese.history['loss'], label='train')\n",
    "# plt.plot(history_siamese.history['val_loss'], label='test')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model_inference.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
