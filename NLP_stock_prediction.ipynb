{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = False\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "MODEL = 'cardiffnlp/twitter-xlm-roberta-base-sentiment' #cardiffnlp/twitter-xlm-roberta-base-sentiment  cardiffnlp/twitter-roberta-base-emotion\n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    if len(password) > 0:\n",
    "        kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "        #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "        #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "        del password;\n",
    "        gc.collect();\n",
    "        get_datasets(kaggle_api_key);\n",
    "    elif len(glob('data/*')) < 4:\n",
    "        cont = input(f\"Password is empty. Press n to cancel or any other key to continue: \")\n",
    "        if cont == 'n':\n",
    "            assert False, \"Exiting program. Please enter a password to continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = emotion_classifier_load(MODEL)\n",
    "\n",
    "# classifier and classify the dataset of analyst ratings and partner headlines\n",
    "if not os.path.exists('data/Text/text_emotion_29.parquet'):\n",
    "    text_df = pd.concat([\n",
    "                    load_file('data/Text/analyst_ratings_processed.parquet').drop(columns=['unnamed: 0']).rename(columns={'title': 'text'}), \n",
    "                    load_file('data/Text/raw_partner_headlines.parquet').drop(columns=['unnamed: 0', 'url', 'publisher']).rename(columns={'headline': 'text'})\n",
    "                    ], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "    text=text_df.text.tolist()\n",
    "    text = classify_text(emotion_classifier, text, MODELNAME=MODEL, bs=100, device=cuda)\n",
    "    text_dict = {'emotion': text}\n",
    "    text_df = pd.concat([text_df, pd.DataFrame(text_dict)], axis=1)\n",
    "    save_file(text_df, 'data/Text/text_emotion_29.parquet')\n",
    "    text_df\n",
    "else:\n",
    "    text_df = load_file('data/Text/text_emotion_29.parquet')\n",
    "    text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_stock_data(ticker=None, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kr stock\n",
    "stock = load_file('data/Stock/KR.parquet')\n",
    "stock\n",
    "text_df, stock = intersect_df(text_df, stock)\n",
    "text_df['date'] = text_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 9 else None).dropna()\n",
    "text_df, stock = intersect_df(text_df, stock)\n",
    "text_df = text_df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by date\n",
    "def daily_stats(df):\n",
    "    \"\"\"Takes in a dataframe and returns the daily std, mean, outliers, and count of the dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to be grouped by date\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            The dataframe with the daily stats\n",
    "    \"\"\"\n",
    "    df = df.drop(['text', 'stock'], axis=1)#.reset_index(drop=False)\n",
    "    df = pd.concat([df, pd.DataFrame(df.emotion.tolist(), index=df.index)], axis=1)\n",
    "    df = df.drop(columns=['emotion'])\n",
    "\n",
    "    # group by date\n",
    "    df = df.groupby('date').agg([lambda x: x.std() if x.count() > 1 else 0, lambda x: x.mean() if x.count() > 1 else 0, lambda x: x.count() if x.count() > 1 else 0])\n",
    "    # flatten the columns\n",
    "    df.columns = ['_'.join([str(x).replace('<lambda_0>', 'std').replace('<lambda_1>', 'mean').replace('<lambda_2>', 'count').replace('<lambda_3>', 'occured') for x in col]) for col in df.columns.values]\n",
    "    df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "text_stats = daily_stats(text_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = search_terms.data[search_terms.data['ticker'] == 'KR'].values.tolist()[0][0:4]\n",
    "search = [x.lower() for x in search if x != None and x != '']\n",
    "search[0] = '#' + search[0]\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_if_error = scrape_tweets(since='2018-11-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file(r'data\\Twitter\\twitter_data___#kr---the kroger co.---william rodney mcmullen---grocery stores.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
