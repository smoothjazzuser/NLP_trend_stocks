{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = False\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "MODEL = 'cardiffnlp/twitter-xlm-roberta-base-sentiment' #cardiffnlp/twitter-xlm-roberta-base-sentiment  cardiffnlp/twitter-roberta-base-emotion\n",
    "print('Available GPU memory:', available_mem(), 'GB')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    if len(password) > 0:\n",
    "        kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "        #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "        #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "        del password;\n",
    "        gc.collect();\n",
    "        get_datasets(kaggle_api_key);\n",
    "    elif len(glob('data/*')) < 4:\n",
    "        cont = input(f\"Password is empty. Press n to cancel or any other key to continue: \")\n",
    "        if cont == 'n':\n",
    "            assert False, \"Exiting program. Please enter a password to continue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_stock_data(ticker='KR', save = True)\n",
    "update_stock_data(ticker='AAPL', save = True)\n",
    "update_stock_data(ticker='TSLA', save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'KR'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "    search = search_terms.data[search_terms.data['ticker'] == 'AAPL'].values.tolist()[0][0:4]\n",
    "    search = [x.lower() for x in search if x != None and x != '']\n",
    "    search[0] = '#' + search[0]\n",
    "    df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "\n",
    "\n",
    "    # search = search_terms.data[search_terms.data['ticker'] == 'TSLA'].values.tolist()[0][0:4]\n",
    "    # search = [x.lower() for x in search if x != None and x != '']\n",
    "    # search[0] = '#' + search[0]\n",
    "    # df_if_error = scrape_tweets(since='2018-01-01', until='2023-04-19', max_tweets=20, update_twitter_data=True, co_list=search)\n",
    "    \n",
    "\n",
    "# how to recover the data if the crawler crashes (example for apple inc.)\n",
    "#tweets_df = pd.DataFrame(df_if_error, columns=['date', 'text', 'username', 'searchterm'])\n",
    "#tweets_df = tweets_df.drop_duplicates(inplace=False, subset=['date', 'text', 'username', 'searchterm']).reset_index(drop=True, inplace=False).dropna(inplace=False)\n",
    "#save_file(tweets_df, r'data\\Twitter\\twitter_data___#aapl---apple inc.---timothy cook---consumer electronics.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_classifier = emotion_classifier_load(MODEL)\n",
    "text_df = classify_twitter_text(save_path='data/Text/text_emotion_29.parquet', model=emotion_classifier, load_path='data/Text/text_emotion_29.parquet')\n",
    "text_df = text_df.drop(columns=['stock','text'], inplace=False).dropna(inplace=False)\n",
    "# len date row chars > 7\n",
    "text_df = text_df[text_df['date'].str.len() > 7]\n",
    "text_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = glob('data/Twitter/twitter_emotion___#*.parquet')\n",
    "index = {x.split(\"#\")[1].split(\"---\")[0]: x for x in index}\n",
    "\n",
    "kr_text =   load_file(index['kr'].replace('emotion', 'data'))\n",
    "aapl_text = load_file(index['aapl'].replace('emotion', 'data'))\n",
    "\n",
    "kr_df   = classify_twitter_text(save_path=index['kr'], model=emotion_classifier, load_path=index['kr'].replace('emotion', 'data'))\n",
    "aapl_df = classify_twitter_text(save_path=index['aapl'], model=emotion_classifier, load_path=index['aapl'].replace('emotion', 'data'))\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kr stock\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "#kr_df, stock_kr = intersect_df(kr_df, stock_kr)\n",
    "kr_df['date'] = kr_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "#kr_df, stock_kr = intersect_df(kr_df, stock_kr)\n",
    "kr_stats = daily_stats(kr_df)\n",
    "\n",
    "stock_kr = load_file('data/Stock/KR.parquet')\n",
    "stock_kr, text_df_kr = intersect_df(stock_kr, text_df)\n",
    "text_stats_kr = daily_stats(text_df_kr)\n",
    "text_df_kr = daily_stats(text_df_kr)\n",
    "\n",
    "\n",
    "\n",
    "# aapl stock\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "#aapl_df, stock_aapl = intersect_df(aapl_df, stock_aapl)\n",
    "aapl_df['date'] = aapl_df['date'].apply(lambda x: arrow.get(str(x)[:10]).format('YYYY-MM-DD') if len(str(x)) > 7 else None).dropna()\n",
    "#aapl_df, stock_aapl = intersect_df(aapl_df, stock_aapl)\n",
    "aapl_stats = daily_stats(aapl_df)\n",
    "\n",
    "stock_aapl = load_file('data/Stock/AAPL.parquet')\n",
    "stock_aapl, text_df_appl = intersect_df(stock_aapl, text_df)\n",
    "text_stats_appl = daily_stats(text_df_appl)\n",
    "text_df_appl = daily_stats(text_df_appl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter to emotion dataframe\n",
    "\n",
    "feature list: \n",
    "\n",
    "              emotions        [std, mean, count],                                 --> no need to normalize\n",
    "              \n",
    "              stock           [open, high, low, close, volume],                   --> normalize percent change\n",
    "\n",
    "              portfolio       [current money, stocks owned, stocks owned value],  --> keep track of portfolio\n",
    "\n",
    "              date            [day of week, holiday, day of month, month]         --> engineer\n",
    "\n",
    "              economic data   [NA]                                                --> normalize \n",
    "\n",
    "output: \n",
    "\n",
    "  volatility:\n",
    "\n",
    "  best action\n",
    "\n",
    "  predicted price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process kroger data. 1) Merge global, stock, and company specific data into a timeseries, with zeros for missing data. 2) Split into train and test sets. 3) Scale the data. 4) Reshape the data into 3D for Resnet, 5) ensure data ordering and labels are correct\n",
    "\n",
    "merged_df_kr = merge_data_timeline(kr_stats, stock_kr, text_stats_kr)\n",
    "y_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "x_cols = merged_df_kr.columns #  if x not in y_cols and x != 'date'... not removed because it is current day's data and we want to predict next day's data\n",
    "X = merged_df_kr[x_cols]\n",
    "Y = merged_df_kr[y_cols]\n",
    "#delete the first row to allign the data\n",
    "Y = Y.drop(Y.index[0]) # y is now the next day's data... e.g. we want to predict tomorrow's stock price\n",
    "X = X.drop(X.index[-1]) \n",
    "\n",
    "# complete timeseries\n",
    "Y = Y.values\n",
    "X = X.values\n",
    "\n",
    "# split into train and test sets by slicing\n",
    "train_size = int(len(X) * 0.90)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, Y_train_kr = X[0:train_size,:], Y[0:train_size,:]\n",
    "X_test, Y_test_kr = X[train_size:len(X),:], Y[train_size:len(Y),:]\n",
    "\n",
    "dates_train = X_train[:,0]\n",
    "dates_test = X_test[:,0]\n",
    "X_train = X_train[:,1:].astype('float32')\n",
    "X_test = X_test[:,1:].astype('float32')\n",
    "\n",
    "# split dim 1 into 2\n",
    "split_into = 2\n",
    "X_train_kr = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_kr = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "#################################################################################################################################\n",
    "# process apple data\n",
    "merged_df_aapl = merge_data_timeline(aapl_stats, stock_aapl, text_stats_appl)\n",
    "y_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "x_cols = merged_df_kr.columns #  if x not in y_cols and x != 'date'... not removed because it is current day's data and we want to predict next day's data\n",
    "X = merged_df_aapl[x_cols]\n",
    "Y = merged_df_aapl[y_cols]\n",
    "#delete the first row to allign the data\n",
    "Y = Y.drop(Y.index[0]) # y is now the next day's data... e.g. we want to predict tomorrow's stock price\n",
    "X = X.drop(X.index[-1]) \n",
    "\n",
    "# complete timeseries\n",
    "Y = Y.values\n",
    "X = X.values\n",
    "\n",
    "# split into train and test sets by slicing\n",
    "train_size = int(len(X) * 0.90)\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, Y_train_aapl = X[0:train_size,:], Y[0:train_size,:]\n",
    "X_test, Y_test_aapl = X[train_size:len(X),:], Y[train_size:len(Y),:]\n",
    "\n",
    "dates_train = X_train[:,0]\n",
    "dates_test = X_test[:,0]\n",
    "X_train = X_train[:,1:].astype('float32')\n",
    "X_test = X_test[:,1:].astype('float32')\n",
    "\n",
    "# split dim 1 into 2\n",
    "split_into = 2\n",
    "X_train_aapl = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_aapl = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_Size = 32\n",
    "X_train_win_kr = sliding_window_view_generator(X_train_kr, window_Size)\n",
    "X_test_win_kr = sliding_window_view_generator(X_test_kr, window_Size)\n",
    "Y_train_win_kr = sliding_window_view_generator(Y_train_kr, 1)\n",
    "Y_test_win_kr = sliding_window_view_generator(Y_test_kr, 1)\n",
    "\n",
    "X_train_win_aapl = sliding_window_view_generator(X_train_aapl, window_Size)\n",
    "X_test_win_aapl = sliding_window_view_generator(X_test_aapl, window_Size)\n",
    "Y_train_win_aapl = sliding_window_view_generator(Y_train_aapl, 1)\n",
    "Y_test_win_aapl = sliding_window_view_generator(Y_test_aapl, 1)\n",
    "\n",
    "\n",
    "model = model((1, window_Size, 216), 2, Y_test_win_kr.shape[1])\n",
    "\n",
    "        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 2\n",
    "early_stopping = True\n",
    "\n",
    "\n",
    "\n",
    "# move to gpu\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_model_perams = None\n",
    "patience = 10\n",
    "hist = {'train_loss': [], 'test_loss': []}\n",
    "# Train on kr data and test on aapl data\n",
    "with tqdm(total=num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "\n",
    "        x = next(X_train_win_kr.next())\n",
    "        y = next(Y_train_win_kr.next())\n",
    "        predictions, actions  = model.forward(x)\n",
    "        loss = model.action(actions, train=True)\n",
    "        loss += criterion(predictions, y)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        hist['train_loss'].append(loss.item())\n",
    "        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {hist['train_loss'][-1]:.4f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        # val step\n",
    "        #if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            for _ in range(len(X_test_win_kr.x)//3):\n",
    "                x = next(X_test_win_aapl.next())\n",
    "                y = next(Y_test_win_aapl.next())\n",
    "                predictions, actions = model.forward(x)\n",
    "                loss += model.action(actions, train=False)\n",
    "                loss += criterion(predictions, y)\n",
    "\n",
    "            loss = loss / len(X_test_win_kr.x)\n",
    "            hist['test_loss'].append(loss.item())\n",
    "\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {hist['train_loss'][-1]:.4f}, Val Loss: {hist['test_loss'][-1]:.4f}\")\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_epoch = epoch\n",
    "                best_model_perams = model.state_dict()\n",
    "            else:\n",
    "                if early_stopping and epoch - best_epoch > patience:\n",
    "                    model.load_state_dict(best_model_perams)\n",
    "                    print(f\"Early stopping at epoch {epoch} with best loss {best_loss} at epoch {best_epoch}, history: {hist['test_loss'][-1]:.4f}\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_model_perams = None\n",
    "patience = 10\n",
    "hist = {'train_loss': [], 'test_loss': []}\n",
    "\n",
    "# trains on apple data and tests on kr data\n",
    "with tqdm(total=num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        x = next(X_train_win_aapl.next())\n",
    "        y = next(Y_train_win_aapl.next())\n",
    "        predictions, actions  = model.forward(x)\n",
    "        loss = model.action(actions, train=True)\n",
    "        loss += criterion(predictions, y)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        hist['train_loss'].append(loss.item())\n",
    "        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {hist['train_loss'][-1]:.4f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        # val step\n",
    "        #if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            for _ in range(len(X_test_win_kr.x)//3):\n",
    "                x = next(X_test_win_kr.next())\n",
    "                y = next(Y_test_win_kr.next())\n",
    "                predictions, actions = model.forward(x)\n",
    "                loss += model.action(actions, train=False)\n",
    "                loss += criterion(predictions, y)\n",
    "\n",
    "            loss = loss / len(X_test_win_kr.x)\n",
    "            hist['test_loss'].append(loss.item())\n",
    "\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {hist['train_loss'][-1]:.4f}, Val Loss: {hist['test_loss'][-1]:.4f}\")\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_epoch = epoch\n",
    "                best_model_perams = model.state_dict()\n",
    "            else:\n",
    "                if early_stopping and epoch - best_epoch > patience:\n",
    "                    model.load_state_dict(best_model_perams)\n",
    "                    print(f\"Early stopping at epoch {epoch} with best loss {best_loss} at epoch {best_epoch}, history: {hist['test_loss'][-1]:.4f}\")\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
