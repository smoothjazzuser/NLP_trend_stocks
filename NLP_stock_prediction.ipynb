{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "import numpy as np\n",
    "import os\n",
    "import gc, os\n",
    "from getpass import getpass\n",
    "from utils import *\n",
    "from models import *\n",
    "import gc\n",
    "import shutil\n",
    "import zipfile\n",
    "import torch\n",
    "import arrow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# currently hardcoded to use GPU in order to identify when cuda is installed incorrectly. The models will not be practical to train on CPU\n",
    "cuda = torch.device(\"cuda\") \n",
    "cpu = torch.device(\"cpu\")\n",
    "# if you don't need the API downloads, you can set this to False\n",
    "use_api = True\n",
    "\n",
    "38\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) \n",
    "print('Available GPU memory:', available_mem(), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Enter password to save/load API keys. This will be used to create/subsequently unlock the encrypted API keys.\n",
    "# 2: if it is the first time running this, this will ask you to enter your username + a single space + your Kaggle API key. (don't use quotes)\n",
    "# 3: any other api keys in future will follow the same format\n",
    "\n",
    "# # may take awhile to load the first time, depending on your internet speeds. \n",
    "# After this first run, you will only need to enter the password to load the api keys \n",
    "# NOTE: Don't delete or move salt.secret as all tokens will become undecryptable. \n",
    "# If you made a mistake or need to retry, you may 1) delete salt.secret to reset everything. 2) delete the specific .secret key to re-enter only that info.\n",
    "if use_api:\n",
    "    # Ask for input of password to save API keys\n",
    "    password = getpass(\"Enter password to save/load API keys: \");\n",
    "    kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "    #td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "    #data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "    del password;\n",
    "    gc.collect();\n",
    "    get_datasets(kaggle_api_key);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers a dataframe of useful terms and info related to every stock in data\\Stock_List.parquet. This may take up to 40 minutes to run the first time (the web scraping is slow due to requests being throttled). Stores results in company_list.pkl\n",
    "search_terms = aquire_stock_search_terms('data/Stock/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for training the sentiment analysis model\n",
    "# ~2m 42s first run, ~25s after (if augment=True, 24 hours, 2m 45s after)\n",
    "\n",
    "if not os.path.exists('models/emotion_classifier.pt'):\n",
    "    classes, train_triplets, test_triplets, x_train, y_train, x_test, y_test = prep_triplet_data(MODEL=f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", augment=True, aug_n = 400000)\n",
    "    ds_train, ds_test = prep_tensor_ds( x_train, y_train, x_test, y_test)\n",
    "    classes_len = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ignore the error if a model weight of size [29, 768] is loaded without weights. This is intentional since we will train the model head as a final step not using contrastive loss.\n",
    "\n",
    "At this point, this model should train to a validation loss of 0. Since we are using contrastive loss (with a margin) the model will stop changing once classes are considered different enough (a perameter that can be set in torch.nn.TripletMarginWithDistanceLoss margine).\n",
    "\n",
    "This step is designed to force the model to encode every class in a manner that is equally easily seperable in the final crossentropy loss (with a set margin of similarity, which can be increased if the classifier struggles).\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists('models/siamese_model.pt'):\n",
    "    \n",
    "    siamese_network_model = siamese_network(classes_len).to(cuda)\n",
    "    siamese_model, history = pre_train_using_siamese(train_triplets, test_triplets, siamese_network_model, epochs=10, classes=classes)\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    torch.save(siamese_model.state_dict(), f'models/siamese_model.pt')\n",
    "\n",
    "    # plot the train and test loss over time on the same plot\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train'], label='pretraining train loss')\n",
    "    plt.plot(history['test'], label='pretraining test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # save plot of train and test loss over time along with history\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    fig.savefig('results/pretrain_emotion_history.jpg')\n",
    "    save_file(history, 'results/pretrain_emotion_history.parquet')\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    siamese_network_model = siamese_network(classes_len).to(cuda)\n",
    "    siamese_network_model.load_state_dict(torch.load(f'models/siamese_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In this step, we finally train the last two weight layers to convert the siamese network into a classifier. Later, we can consider unfreezing a few of the earlier layers to improve performance with a lower learning rate.\"\"\"\n",
    "\n",
    "if not os.path.exists('models/emotion_classifier.pt'):\n",
    "    model = classify_single_input(siamese_network_model)\n",
    "    model = model.to(cuda)\n",
    "    model, history = train_emotion_classifier(model, ds_train, ds_test, epochs=2)\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    torch.save(model.state_dict(), f'models/emotion_classifier.pt')\n",
    "\n",
    "    # plot the train and test loss over time on the same plot\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train'], label='train loss')\n",
    "    plt.plot(history['test'], label='test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # save plot of train and test loss over time along with history\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    fig.savefig('results/emotion_history.jpg')\n",
    "    save_file(history, 'results/emotion_history.parquet')\n",
    "    plt.close(fig)\n",
    "else:\n",
    "    model = classify_single_input(siamese_network_model)\n",
    "    model = model.to(cuda)\n",
    "    model.load_state_dict(torch.load(f'models/emotion_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################# Experiments with web scraping. Unfinished, so ignore if it does not work #############################\n",
    "from newspaper import Article\n",
    "import newspaper\n",
    "import scrapy\n",
    "import stweet as st\n",
    "import requests\n",
    "from stweet.search_runner import SearchRunContext\n",
    "import tor_python_easy\n",
    "import io\n",
    "#arrow time\n",
    "# import urllib.request as urllib2\n",
    "import arrow\n",
    "import json\n",
    "import os\n",
    "# import yaml\n",
    "# INSTALL DOCKER COMPOSE\n",
    "\n",
    "# if not os.path.exists('docker-compose.yml'):\n",
    "#     url = 'https://raw.githubusercontent.com/markowanga/tor-python-easy/main/docker-compose.yml'\n",
    "#     filename = 'docker-compose.yml'\n",
    "#     urllib2.urlretrieve(url, filename);\n",
    "#     with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#     config['services']['torproxy']['environment'][0]=f'PASSWORD={os.urandom(32).hex()}';\n",
    "#     #with open('docker-compose.yml', 'w') as file:\n",
    "#        # yaml.dump(config, file);\n",
    "\n",
    "# with open('docker-compose.yml', 'r') as file:\n",
    "#         config = yaml.safe_load(file);\n",
    "#         tor_pass = config['services']['torproxy']['environment'][0].split('=')[1]; \n",
    "\n",
    "# from tor_python_easy.tor_control_port_client import TorControlPortClient\n",
    "# from tor_python_easy.tor_socks_get_ip_client import TorSocksGetIpClient\n",
    "# proxy_config = {'http': 'socks5://localhost:9050','https': 'socks5://localhost:9050',}\n",
    "# ip_client = TorSocksGetIpClient(proxy_config)\n",
    "# tor_control_port_client = TorControlPortClient(control_address='localhost', control_port=9051, control_password=tor_pass)\n",
    "\n",
    "def url_fetch(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text, article.title, article.publish_date, article.authors, article.summary, article.keywords\n",
    "\n",
    "def crawl_url(url):\n",
    "    page = newspaper.build(url, memoize_articles=False)\n",
    "    articles = page.articles\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = arrow.get('2021-01-01').datetime\n",
    "until = arrow.get('2021-02-01').datetime\n",
    "exact_words = None\n",
    "all_words = None\n",
    "any_word = '#apple'\n",
    "\n",
    "# web_client = st.DefaultTwitterWebClientProvider.get_web_client_preconfigured_for_tor_proxy(\n",
    "#     socks_proxy_url='socks5://localhost:9050',\n",
    "#     control_host='localhost',\n",
    "#     control_password=tor_pass,\n",
    "#     control_port=9051,\n",
    "# )\n",
    "\n",
    "\n",
    "collect1 = st.CollectorRawOutput()\n",
    "collect2 = st.CollectorRawOutput()\n",
    "context = SearchRunContext()\n",
    "\n",
    "#since = arrow.get(since).datetime\n",
    "#until = arrow.get(until).datetime\n",
    "\n",
    "tweets_task = st.SearchTweetsTask(since=since, until=until, any_word=any_word, exact_words=exact_words,all_words=all_words, tweets_limit=50, replies_filter=True)\n",
    "runner = st.TweetSearchRunner(search_tweets_task=tweets_task,tweet_raw_data_outputs=[collect1],user_raw_data_outputs=[collect2], web_client=None, search_run_context=context)\n",
    "\n",
    "runner.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.search_run_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = collect1.get_raw_list()\n",
    "\n",
    "l2 = collect2.get_raw_list()\n",
    "\n",
    "len(l), len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = [value for key,value in json.loads(l[0].to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']]\n",
    "d[0] = arrow.get(d[0], 'ddd MMM DD HH:mm:ss Z YYYY').datetime.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set([[value for key,value in json.loads(x.to_json_line())['raw_value'].items() if key in ['full_text','created_at','id']][1] for x in l])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
