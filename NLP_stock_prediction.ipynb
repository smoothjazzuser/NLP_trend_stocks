{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python packages needed: compress_pickle[lz4] pandas numpy yahooquery seaborn\n",
    "\n",
    "If for some reason you wish to download the data yourself, you can do so by following these steps:\n",
    "\n",
    "1) Download and extract https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset\n",
    "\n",
    "2) From the stock dataset, copy \"Stock_List.csv\" and \"Stocks/*\" to \"data/\". You can delete the other files as they are not used (at least yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    # set pandas to use pyarrow')\n",
    "    pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "    import numpy as np\n",
    "    import seaborn\n",
    "    from glob import glob\n",
    "    from compress_pickle import dump, load\n",
    "    import os\n",
    "    from yahooquery import Ticker\n",
    "    import timeit\n",
    "    import time\n",
    "    import gc, os\n",
    "    import datetime\n",
    "    from getpass import getpass\n",
    "    from shutil import rmtree\n",
    "    from utils import fernet_key_encryption, aquire_stock_search_terms as aquire_terms, get_macroeconomic_data as macro_data, download_datasets, load_file, interpolate_months_to_days, intersect_df, parse_emotion_dataframes\n",
    "except ModuleNotFoundError as e:\n",
    "    print(e)\n",
    "    print('Please install the missing module(s)')\n",
    "    print(\"pip install compress_pickle[lz4] pandas numpy yahooquery cryptography seaborn kaggle pyarrow\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this first run, you will only need to enter the password to load the api keys.\n",
    "# If you need to change the keys or password, delete the relevent .secret keys file and run this section again.\n",
    "# salt.secret is a non-sensitive file that is used to both generate the encryption key as well as decryption. If this key is lost, the encrypted files are lost and you will need to re-enter the api keys.\n",
    "\n",
    "# Ask for input of password to save API keys\n",
    "password = getpass(\"Enter password to save/load API keys: \");\n",
    "\n",
    "kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "#td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "\n",
    "#data.nasdaq.com api key\n",
    "data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "\n",
    "del password;\n",
    "gc.collect();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username, password = kaggle_api_key.split(' ');\n",
    "os.environ['KAGGLE_USERNAME'] = username;\n",
    "os.environ['KAGGLE_KEY'] = password;\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd();\n",
    "\n",
    "# download the various kaggle datasets\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/sarthmirashi07/us-macroeconomic-data', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'US_macroeconomics.csv': 'macro/US_macroeconomics.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Macro')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'us-stock-dataset/Stock_List.csv': 'Stock_List.csv', 'us-stock-dataset/Data/Stocks': 'Stocks'}, \n",
    "        delete=True,\n",
    "        dest_name='Stocks')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/mathurinache/goemotions',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'goemotions.csv': 'Emotions/goemotions.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/parulpandey/emotion-dataset',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'training.csv': 'Emotions/training.csv', 'validation.csv': 'Emotions/validation.csv', 'test.csv': 'Emotions/test.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/kosweet/cleaned-emotion-extraction-dataset-from-twitter',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'dataset(clean).csv': 'Emotions/dataset(clean).csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'raw_partner_headlines.csv': 'Text/raw_partner_headlines.csv', 'raw_analyst_ratings.csv': 'Text/raw_analyst_ratings.csv', 'analyst_ratings_processed.csv': 'Text/analyst_ratings_processed.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Text')\n",
    "\n",
    "\n",
    "\n",
    "# clear the username and key from the environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = \"\" \n",
    "os.environ['KAGGLE_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the company info for all the ticker symbols and return a dataframe with relevant search terms for each company.\n",
    "# If the stocks dataset is updated on kaggle, compank_list.pkl needs to be deleted and this run again if the symbols have changed. It would be more efficient to manually pull the new data ourselves.\n",
    "search_terms = aquire_terms('data/Stocks/')\n",
    "search_terms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gether historical macroeconomic data from different sources.\n",
    "#macro_data = macro_data('data/Macro/')\n",
    "\n",
    "df1 = load_file('data/Macro/US_macroeconomics.parquet')\n",
    "df2 = load_file('data\\Stocks\\AAPL.parquet')\n",
    "\n",
    "#for col in [x for x in df.columns if x not in [\"CPI\", \"date\", \"Unemp_rate\", \"mortgage_rate\"]]:\n",
    "    #df[col] = df[col].div(df['CPI'])\n",
    "    \n",
    "\n",
    "\n",
    "df1, df2 = intersect_df(df1, df2, interpolate_to_days=True, extend_trend_to_today=False) # extend_trend_to_today should only be used when the macro data is recent.\n",
    "df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still needs to be checked for accuracy/cleannes after merging the datasets.\n",
    "emotion_df = parse_emotion_dataframes([0, 1, 2, 3, 4])\n",
    "emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df.value_counts([x for x in emotion_df.columns if x != 'text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
