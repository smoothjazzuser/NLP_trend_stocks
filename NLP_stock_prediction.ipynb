{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "        Prototyping notebook for predicting stock volaitility, prices, etc using extra data from web trends, news, etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python packages needed: compress_pickle[lz4] pandas numpy yahooquery cryptography seaborn kaggle pyarrow transformers fasttext tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import pandas as pd\n",
    "    pd.set_option('io.parquet.engine', 'pyarrow')\n",
    "    import numpy as np\n",
    "    import seaborn\n",
    "    from glob import glob\n",
    "    from compress_pickle import dump, load\n",
    "    from matplotlib import pyplot as plt\n",
    "    import os\n",
    "    from yahooquery import Ticker\n",
    "    import timeit\n",
    "    import time\n",
    "    import gc, os\n",
    "    import datetime\n",
    "    from getpass import getpass\n",
    "    from shutil import rmtree\n",
    "    from utils import fernet_key_encryption, aquire_stock_search_terms as aquire_terms, get_macroeconomic_data as macro_data, download_datasets, save_file, load_file, interpolate_months_to_days, intersect_df, parse_emotion_dataframes, get_emotion_df, create_triplets\n",
    "    from models import siamese_model, triplet_loss\n",
    "    import transformers\n",
    "    from tqdm.notebook import tqdm\n",
    "    import gc\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models, preprocessing, callbacks, optimizers, losses, metrics\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "    print(e)\n",
    "    print('Please install the missing module(s)')\n",
    "    print(\"pip install compress_pickle[lz4] pandas numpy yahooquery cryptography seaborn kaggle pyarrow transformers fasttext tensorflow==2.10.1\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this first run, you will only need to enter the password to load the api keys.\n",
    "# If you need to change the keys or password, delete the relevent .secret keys file and run this section again.\n",
    "# salt.secret is a non-sensitive file that is used to both generate the encryption key as well as decryption. If this key is lost, the encrypted files are lost and you will need to re-enter the api keys.\n",
    "\n",
    "# Ask for input of password to save API keys\n",
    "password = getpass(\"Enter password to save/load API keys: \");\n",
    "\n",
    "kaggle_api_key = fernet_key_encryption(password, 'Kaggle');\n",
    "#td_ameritrade_api_key = fernet_key_encryption(password, 'TD_Ameritrade')\n",
    "\n",
    "#data.nasdaq.com api key\n",
    "data_nasdaq_key = fernet_key_encryption(password, 'Nasdaq');\n",
    "\n",
    "del password;\n",
    "gc.collect();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username, password = kaggle_api_key.split(' ');\n",
    "os.environ['KAGGLE_USERNAME'] = username;\n",
    "os.environ['KAGGLE_KEY'] = password;\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd();\n",
    "\n",
    "# download the various kaggle datasets\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/sarthmirashi07/us-macroeconomic-data', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'US_macroeconomics.csv': 'macro/US_macroeconomics.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Macro')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/footballjoe789/us-stock-dataset', \n",
    "        kaggle_api_key, \n",
    "        files_to_move={'us-stock-dataset/Stock_List.csv': 'Stock_List.csv', 'us-stock-dataset/Data/Stocks': 'Stocks'}, \n",
    "        delete=True,\n",
    "        dest_name='Stocks')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/mathurinache/goemotions',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'goemotions.csv': 'Emotions/goemotions.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/parulpandey/emotion-dataset',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'training.csv': 'Emotions/training.csv', 'validation.csv': 'Emotions/validation.csv', 'test.csv': 'Emotions/test.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/kosweet/cleaned-emotion-extraction-dataset-from-twitter',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'dataset(clean).csv': 'Emotions/dataset(clean).csv'},\n",
    "        delete=True,\n",
    "        dest_name='Emotions')\n",
    "\n",
    "download_datasets(\n",
    "        'https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests',\n",
    "        kaggle_api_key,\n",
    "        files_to_move={'raw_partner_headlines.csv': 'Text/raw_partner_headlines.csv', 'raw_analyst_ratings.csv': 'Text/raw_analyst_ratings.csv', 'analyst_ratings_processed.csv': 'Text/analyst_ratings_processed.csv'},\n",
    "        delete=True,\n",
    "        dest_name='Text')\n",
    "\n",
    "\n",
    "\n",
    "# clear the username and key from the environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = \"\" \n",
    "os.environ['KAGGLE_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the company info for all the ticker symbols and return a dataframe with relevant search terms for each company.\n",
    "# If the stocks dataset is updated on kaggle, compank_list.pkl needs to be deleted and this run again if the symbols have changed. It would be more efficient to manually pull the new data ourselves.\n",
    "search_terms = aquire_terms('data/Stocks/')\n",
    "search_terms.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_file('data/Macro/US_macroeconomics.parquet')\n",
    "df2 = load_file('data\\Stocks\\AAPL.parquet')\n",
    "\n",
    "df1, df2 = intersect_df(df1, df2, interpolate_to_days=True, extend_trend_to_today=False) # extend_trend_to_today should only be used when the macro data is recent.\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = get_emotion_df()\n",
    "x = np.array([x for x in emotion_df['text'].values])\n",
    "y = emotion_df[[x for x in emotion_df.columns if x != 'text']].values.astype('float32')\n",
    "x_shape = x[0].shape\n",
    "label_shape = y[0].shape\n",
    "x.shape, y.shape\n",
    "del emotion_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "triplet_train = create_triplets(x_train, y_train, batch_size=32, shuffle=True, seed=42)\n",
    "triplet_test = create_triplets(x_test, y_test, batch_size=10, shuffle=True, seed=42)\n",
    "del x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Given that the text emotion dataset is highly imbalanced, we will construct a siamese model to create an embedding that is seperable between the ~30 classes of emotions. This will give the classes equal probbaility of being accessed as well as serve as N^3 dataset augmentation. We can also easily compare the accuracy of the siamese network by adding a different head to the model and training it on a simpler ubalanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_siamese, model_encoder, model_inference = siamese_model(x_shape, label_shape)\n",
    "model_siamese.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_siamese = model_siamese.fit(triplet_train.generator(), steps_per_epoch=triplet_train.num_batches, epochs=100, validation_data=triplet_test.generator(), validation_steps=triplet_test.num_batches, callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "\n",
    "# freeze the layers\n",
    "for layer in model_siamese.layers:\n",
    "    layer.trainable = False\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_siamese.history['loss'], label='train')\n",
    "plt.plot(history_siamese.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_inference.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test), callbacks=[callbacks.EarlyStopping(patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee6f859ee4f4f4627489a966496f20487f69a100034e62284b5cd55bca112eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
